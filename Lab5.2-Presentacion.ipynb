{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f2b6da609e704f94a7612894f1f0d838",
    "deepnote_app_block_group_id": null,
    "deepnote_app_block_order": 0,
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<div style=\"border-radius: 5px; padding: 1rem; margin-bottom: 1rem\">\n",
    "<img src=\"https://www.prototypesforhumanity.com/wp-content/uploads/2022/11/LOGO_UTEC_.png\" alt=\"Banner\" width=\"150\" />   \n",
    " </div>\n",
    "\n",
    "# Laboratorio 5.2: Similitud de Coseno e Indice Invertido\n",
    "\n",
    "> **Daniel Casquino**  \n",
    "> **Jesús Valentín Niño Castañeda**\n",
    "\n",
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import math\n",
    "import psycopg2\n",
    "import warnings\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import bisect\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "stemmer = nltk.stem.SnowballStemmer(\"spanish\")\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Omitir advertencias de pandas sobre SQLAlchemy\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"pandas only supports SQLAlchemy connectable\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "\n",
    "def connect_db():\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"Lab5.1\",\n",
    "        user=\"postgres\",\n",
    "        password=\"postgres\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\",\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "\n",
    "def fetch_document(id: int):\n",
    "    conn = connect_db()\n",
    "    query = f\"SELECT * FROM noticias WHERE noticias.id = {id};\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df[\"bag_of_words\"] = df[\"bag_of_words\"].apply(\n",
    "        lambda x: json.loads(x) if isinstance(x, str) else x\n",
    "    )\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_data():\n",
    "    conn = connect_db()\n",
    "    query = \"SELECT id, contenido, bag_of_words FROM noticias;\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df[\"bag_of_words\"] = df[\"bag_of_words\"].apply(\n",
    "        lambda x: json.loads(x) if isinstance(x, str) else x\n",
    "    )\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_stopwords():\n",
    "    conn = connect_db()\n",
    "    query = \"SELECT word FROM stopwords;\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    stopword_list = df[\"word\"].tolist()\n",
    "    return stopword_list\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-záéíóúñü\\s]\", \"\", text)\n",
    "    tokens = nltk.word_tokenize(text, \"spanish\")\n",
    "    filtered = [token for token in tokens if token not in stopwords]\n",
    "    stem = [stemmer.stem(w) for w in filtered]\n",
    "    return stem\n",
    "\n",
    "\n",
    "def compute_bow(text):\n",
    "    processed_text = preprocess(text)\n",
    "    bow = dict()\n",
    "    for word in processed_text:\n",
    "        if word in bow:\n",
    "            bow[word] += 1\n",
    "        else:\n",
    "            bow[word] = 1\n",
    "    return bow\n",
    "\n",
    "\n",
    "def update_bow_in_db(dataframe):\n",
    "    conn = connect_db()\n",
    "    cursor = conn.cursor()\n",
    "    for _, row in dataframe.iterrows():\n",
    "        bow = compute_bow(row[\"contenido\"])\n",
    "        query = \"UPDATE noticias SET bag_of_words = %s WHERE id = %s;\"\n",
    "        cursor.execute(query, (json.dumps(bow), row[\"id\"]))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "stopwords = fetch_stopwords()\n",
    "noticias_df = fetch_data()\n",
    "\n",
    "# grammar:\n",
    "# S -> S A B | keyword\n",
    "# B -> S | keyword\n",
    "# A -> OR | AND | AND-NOT\n",
    "\n",
    "\n",
    "def apply_boolean_query(query, table=\"noticias\"):\n",
    "    tokens = preprocess(query)\n",
    "    if (tokens[0] == \"or\" or tokens[0] == \"and\" or tokens[0] == \"and-not\"):\n",
    "        tokens = tokens[1:]\n",
    "    # simple sequential parser:\n",
    "    expects_keyword = True\n",
    "    final_query = f\"SELECT * FROM {table} WHERE bag_of_words ? \"\n",
    "\n",
    "    for w in tokens:\n",
    "        if expects_keyword:\n",
    "            final_query += \"'\" + w + \"'\"\n",
    "            expects_keyword = False\n",
    "        else:\n",
    "            w = w.lower()\n",
    "            if w == \"or\":\n",
    "                final_query += \" OR bag_of_words ? \"\n",
    "            elif w == \"and\":\n",
    "                final_query += \" AND bag_of_words ? \"\n",
    "            elif w == \"and-not\":\n",
    "                final_query += \" AND NOT bag_of_words ? \"\n",
    "            else:\n",
    "                print(\"ERROR: INVALID QUERY, UNRECOGNIZED OPERATOR \" + w)\n",
    "                return pd.DataFrame()\n",
    "            expects_keyword = True\n",
    "\n",
    "    if expects_keyword is True:\n",
    "        print(\"ERROR: EXPECTED KEYWORD AT END\")\n",
    "\n",
    "    final_query += \";\"\n",
    "\n",
    "    # actual query time\n",
    "    conn = connect_db()\n",
    "    df = pd.read_sql(final_query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def bows_to_vectors(df):\n",
    "    vocab = set()\n",
    "    df_counts = dict()\n",
    "    for bow in df[\"bag_of_words\"]:\n",
    "        for word in bow:\n",
    "            vocab.add(word)\n",
    "            if word in df_counts:\n",
    "                df_counts[word] += 1\n",
    "            else:\n",
    "                df_counts[word] = 1\n",
    "    \n",
    "    vocab = sorted(vocab)\n",
    "    N = len(df)\n",
    "    idf = {word: math.log(N / df_counts[word]) for word in vocab}\n",
    "\n",
    "    tf = []\n",
    "    for bow in df[\"bag_of_words\"]:\n",
    "        vector = [1 + math.log(bow.get(word, 0)) if bow.get(word, 0) > 0 else 0 for word in vocab]\n",
    "        tf.append(vector)\n",
    "    \n",
    "    tf_idf = []\n",
    "    for tf_vector in tf:\n",
    "        tf_idf_vector = [t * idf[word] for word, t in zip(vocab, tf_vector)]\n",
    "        tf_idf.append(tf_idf_vector)\n",
    "    \n",
    "    normalized_tf_idf = []\n",
    "    for val in tf_idf:\n",
    "        mag = math.sqrt(sum([t**2 for t in val]))\n",
    "        if mag > 0:\n",
    "            val = [t / mag for t in val]\n",
    "        normalized_tf_idf.append(val)\n",
    "\n",
    "    df[\"tf_idf\"] = normalized_tf_idf\n",
    "\n",
    "    return df, vocab, idf\n",
    "\n",
    "def calculate_similarity(query, df, vocab, idf):\n",
    "    query_bow = compute_bow(query)\n",
    "    query_tf = [1 + math.log(query_bow.get(word, 0)) if query_bow.get(word, 0) > 0 else 0 for word in vocab]\n",
    "    query_tf_idf = [t * idf[word] for word, t in zip(vocab, query_tf)]\n",
    "    query_mag = math.sqrt(sum([t**2 for t in query_tf_idf]))\n",
    "    query_normalized = [t / query_mag for t in query_tf_idf]\n",
    "    df[\"similarity\"] = df[\"tf_idf\"].apply(lambda x: sum([a * b for a, b in zip(x, query_normalized)]))\n",
    "    df = df.sort_values(by=\"similarity\", ascending=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def search(query, top_k=5, table=\"noticias\"):\n",
    "    processed_query = preprocess(query)\n",
    "    # build query from stemmed\n",
    "    final_query = \"\"\n",
    "    for i, s in enumerate(processed_query):\n",
    "        if i < len(processed_query) - 1:\n",
    "            final_query += f\"{s} OR \"\n",
    "        else:\n",
    "            final_query += s\n",
    "    df = apply_boolean_query(final_query, table)\n",
    "    df, vocab, idf = bows_to_vectors(df)\n",
    "    df = calculate_similarity(query, df, vocab, idf)\n",
    "    return df.head(top_k)\n",
    "\n",
    "\n",
    "def test_lab_5_1():\n",
    "    test_queries = [\n",
    "        \"ingeniería OR software AND desarrollo\",\n",
    "        \"inteligencia AND artificial AND-NOT humano\",\n",
    "        \"ciencia OR tecnología AND-NOT medicina\",\n",
    "        \"educación AND aprendizaje OR enseñanza\",\n",
    "        \"computción AND matemática AND-NOT física\",\n",
    "        \"derecho OR leyes AND justicia\",\n",
    "        \"historia OR geografía AND-NOT política\",\n",
    "        \"arte OR cultura AND-NOT entretenimiento\",\n",
    "        \"musica AND danza OR teatro\",\n",
    "        \"salud AND bienestar OR medicina\",\n",
    "    ]\n",
    "\n",
    "    tables = [\"noticias\", \"noticias600\", \"noticias300\", \"noticias150\"]\n",
    "    results = []\n",
    "    time_totals = {\n",
    "        \"noticias\": [],\n",
    "        \"noticias150\": [],\n",
    "        \"noticias300\": [],\n",
    "        \"noticias600\": [],\n",
    "    }\n",
    "\n",
    "    for table in tables:\n",
    "        for query in test_queries:\n",
    "            start = time.time()\n",
    "            df = apply_boolean_query(query, table)\n",
    "            end = time.time()\n",
    "            elapsed_ms = (end - start) * 1000  # Convertir a milisegundos\n",
    "            time_totals[table].append(elapsed_ms)\n",
    "            results.append(\n",
    "                {\n",
    "                    \"tabla\": table,\n",
    "                    \"query\": query,\n",
    "                    \"tiempo_ms\": elapsed_ms,\n",
    "                    \"resultados\": len(df),\n",
    "                }\n",
    "            )\n",
    "            print(f\"{table} | {query} | {elapsed_ms:.2f} ms | {len(df)} resultados\")\n",
    "\n",
    "    print(\"\\nPromedio de tiempo por tabla:\")\n",
    "    for table in tables:\n",
    "        times = time_totals[table]\n",
    "        avg = sum(times) / len(times) if times else 0\n",
    "        print(f\"{table}: {avg:.2f} ms\")\n",
    "\n",
    "    with open(\"resultados.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Resultados guardados en resultados.csv\")\n",
    "\n",
    "\n",
    "def test_lab_5_2():\n",
    "    test_queries = [\n",
    "        \"¿Cuáles son las últimas innovaciones en la banca digital y la tecnología financiera?\",\n",
    "        \"evolución de la inflación y el crecimiento de la economía en los últimos años\",\n",
    "        \"avances sobre sostenibilidad y energías renovables para el medio ambiente\",\n",
    "    ]\n",
    "\n",
    "    for query in test_queries:\n",
    "        results = search(query, top_k=3)\n",
    "        print(f\"Probando consulta: '{query}'\")\n",
    "        for _, row in results.iterrows():\n",
    "            print(f\"\\nID: {row['id']}\")\n",
    "            print(f\"Similitud: {row['similarity']:.3f}\")\n",
    "            print(f\"Texto: {row['contenido'][:200]}...\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# noticias_df = fetch_data()\n",
    "# stopwords = fetch_stopwords()\n",
    "# print(noticias_df)\n",
    "# print(stopwords)\n",
    "# update_bow_in_db(noticias_df)\n",
    "\n",
    "# test_lab_5_1()\n",
    "test_lab_5_2()\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.index = {}  # key should be a word, value should be a list of doc_id, word freq in doc tuples in descending order\n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "\n",
    "    def showDocument(self, id: int):\n",
    "        df = fetch_document(id)\n",
    "        return df[\"contenido\"]\n",
    "\n",
    "    def insert_index_sorted(self, word, id, freq):\n",
    "        if word not in self.index:\n",
    "            self.index[word] = []\n",
    "        bisect.insort(self.index[word], (id, freq))\n",
    "\n",
    "    def update_idf(self, word):\n",
    "        if word not in self.idf:\n",
    "            self.idf[word] = 0\n",
    "        self.idf[word] += 1\n",
    "\n",
    "    def update_length(self, id, freq):\n",
    "        if id not in self.length:\n",
    "            self.length[id] = 0\n",
    "        self.length[id] = math.sqrt(self.length[id] ** 2 + freq**2)\n",
    "\n",
    "    def build_from_db(self):\n",
    "        # Leer desde PostgreSQL todos los bag of words\n",
    "        # Construir el índice invertido, el idf y la norma (longitud) de cada documento\n",
    "\n",
    "        \"\"\"\n",
    "        store id of doc instead of literally doc1\n",
    "        indice  = {\n",
    "            \"word1\": [(\"doc1\", tf1), (\"doc2\", tf2), (\"doc3\", tf3)],\n",
    "            \"word2\": [(\"doc2\", tf2), (\"doc4\", tf4)],\n",
    "            \"word3\": [(\"doc3\", tf3), (\"doc5\", tf5)],\n",
    "        }\n",
    "        idf  = {\n",
    "            \"word1\": 3,\n",
    "            \"word2\": 2,\n",
    "            \"word3\": 2,\n",
    "        }\n",
    "        length = {\n",
    "            \"doc1\": 15.5236,\n",
    "            \"doc2\": 10.5236,\n",
    "            \"doc3\": 5.5236,\n",
    "        }\n",
    "        \"\"\"\n",
    "        df = fetch_data()\n",
    "        # df has id, contenido, bag_of_words\n",
    "        for _, row in df.iterrows():\n",
    "            bow = row[\"bag_of_words\"]\n",
    "            for word, freq in bow.items():\n",
    "                self.insert_index_sorted(word, row[\"id\"], freq)\n",
    "                self.update_idf(word)\n",
    "                self.update_length(row[\"id\"], freq)\n",
    "\n",
    "    def L(self, word) -> list[tuple[str, int]]:\n",
    "        return self.index.get(word, [])\n",
    "\n",
    "    def cosine_search(self, query, top_k=5):\n",
    "        # No es necesario usar vectores numericos del tamaño del vocabulario\n",
    "        # Guiarse del algoritmo visto en clase\n",
    "        # Se debe calcular el tf-idf de la query y de cada documento\n",
    "\n",
    "        score = {}\n",
    "        vectors = {}  # id, list of frequencies\n",
    "        query_bow = compute_bow(query)  # get stemmed tokens\n",
    "        tokens = list(query_bow.keys())\n",
    "\n",
    "        query_vec = []\n",
    "\n",
    "        N = len(self.length)  # n total documents\n",
    "\n",
    "        for col, tok in enumerate(tokens):\n",
    "            # progressive query vector building\n",
    "            tf = query_bow[tok]  # tf is tok's raw frequency in query\n",
    "            tf = 1 + np.log(tf) if tf > 0 else 0\n",
    "            df = self.idf[tok]  # n of documents that contain tok\n",
    "            idf = np.log(N / df)\n",
    "            query_vec.append(tf * idf)\n",
    "\n",
    "            matching_docs = self.L(tok)  # returns list of docs that contain word\n",
    "            for id, raw_freq in matching_docs:\n",
    "                # init vector if not present\n",
    "                if id not in vectors:\n",
    "                    vectors[id] = [0] * len(tokens)\n",
    "                tf = 1 + np.log(raw_freq) if raw_freq > 0 else 0\n",
    "                vectors[id][col] = tf * idf  # calc tf-idf here\n",
    "\n",
    "        query_mag = np.linalg.norm(query_vec)\n",
    "\n",
    "        # now calculate mag of all documents and also cos(angle)\n",
    "        for id, vec in vectors.items():\n",
    "            curr_mag = np.linalg.norm(vec)\n",
    "            score[id] = np.dot(query_vec, vec) / (query_mag * curr_mag)\n",
    "\n",
    "        # Ordenar el score resultante de forma descendente\n",
    "        result = sorted(score.items(), key=lambda tup: tup[1], reverse=True)\n",
    "        # retornamos los k documentos mas relevantes (de mayor similitud a la query)\n",
    "        return result[:top_k]\n",
    "\n",
    "def cosine_similarity_using_index():\n",
    "    idx = InvertedIndex()\n",
    "    idx.build_from_db()\n",
    "\n",
    "    test_queries = [\n",
    "        (\n",
    "            \"¿Cuáles son las últimas innovaciones en la banca digital y la tecnología financiera?\",\n",
    "            4,\n",
    "        ),\n",
    "        (\n",
    "            \"evolución de la inflación y el crecimiento de la economía en los últimos años\",\n",
    "            10,\n",
    "        ),\n",
    "        (\n",
    "            \"avances sobre sostenibilidad y energías renovables para el medio ambiente\",\n",
    "            8,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for test in test_queries:\n",
    "        results = idx.cosine_search(test[0], test[1])\n",
    "        print(f\"Top {test[1]} documentos más similares:\")\n",
    "        for doc_id, score in results:\n",
    "            print(f\"Doc {doc_id}: {score:.3f}: \", idx.showDocument(doc_id))\n",
    "\n",
    "#cosine_similarity_using_index()\n",
    "\n",
    "def AND(list1, list2):\n",
    "    i = j = 0\n",
    "    result = list()\n",
    "    while (i < len(list1) and j < len(list2)):\n",
    "        if (list1[i] == list2[j]):\n",
    "            result.append(list1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif (list1[i] < list2[j]):\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return result\n",
    "\n",
    "def OR(list1, list2):\n",
    "    i = j = 0\n",
    "    result = set()\n",
    "    for id in list1:\n",
    "        result.add(id)\n",
    "    for id in list2:\n",
    "        result.add(id)\n",
    "    result = sorted(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def AND_NOT(list1, list2):\n",
    "    # Implementar la diferencia de dos listas O(n +m)\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_app_clear_outputs": false,
  "deepnote_app_execution_enabled": false,
  "deepnote_app_layout": "powerful-article",
  "deepnote_app_reactivity_enabled": false,
  "deepnote_app_run_on_input_enabled": false,
  "deepnote_app_run_on_load_enabled": false,
  "deepnote_notebook_id": "06ed60ec5c244088a1d0a36a2bfe5404",
  "deepnote_persisted_session": {
   "createdAt": "2024-05-16T15:15:24.980Z"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
